{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ops import conv2d, deconv2d, lrelu, fc, batch_norm, init_embedding, embedding_lookup\n",
    "from dataset import TrainDataProvider, InjectDataProvider, NeverEndingLoopingProvider\n",
    "from utils import scale_back, merge, save_concat_images\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickled total 86423 examples\n",
      "unpickled total 21677 examples\n",
      "filter by label -> range(0, 23)\n",
      "train examples -> 43193, val examples -> 10857\n",
      "1350\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/'\n",
    "batch_size = 32\n",
    "\n",
    "# 23가지 폰트만 가져오기\n",
    "data_provider = TrainDataProvider(data_dir, filter_by=range(23))\n",
    "total_batches = data_provider.compute_total_batch_num(batch_size)\n",
    "print(total_batches)\n",
    "\n",
    "train_batch_iter = data_provider.get_train_iter(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 torch.Size([32, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for bid, batch in enumerate(train_batch_iter):\n",
    "    labels, codes, batch_images = batch\n",
    "    count += 1\n",
    "    break\n",
    "# print(count)\n",
    "print(len(labels), batch_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tensorflow code test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(x, is_training=True, epsilon=1e-5, decay=0.9, scope=\"batch_norm\"):\n",
    "    return tf.contrib.layers.batch_norm(x, decay=decay, updates_collections=None, epsilon=epsilon,\n",
    "                                        scale=True, is_training=is_training, scope=scope)\n",
    "\n",
    "\n",
    "def conv2d(x, output_filters, kh=5, kw=5, sh=2, sw=2, stddev=0.02, scope=\"conv2d\"):\n",
    "    with tf.variable_scope(scope):\n",
    "        shape = x.shape#.get_shape().as_list()\n",
    "        W = tf.get_variable('W', [kh, kw, shape[-1], output_filters],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        Wconv = tf.nn.conv2d(x, W, strides=[1, sh, sw, 1], padding='SAME')\n",
    "\n",
    "        biases = tf.get_variable('b', [output_filters], initializer=tf.constant_initializer(0.0))\n",
    "        Wconv_plus_b = tf.reshape(tf.nn.bias_add(Wconv, biases), Wconv.get_shape())\n",
    "\n",
    "        return Wconv_plus_b\n",
    "\n",
    "\n",
    "def deconv2d(x, output_shape, kh=5, kw=5, sh=2, sw=2, stddev=0.02, scope=\"deconv2d\"):\n",
    "    with tf.variable_scope(scope):\n",
    "        # filter : [height, width, output_channels, in_channels]\n",
    "        input_shape = x.get_shape().as_list()\n",
    "        W = tf.get_variable('W', [kh, kw, output_shape[-1], input_shape[-1]],\n",
    "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "        deconv = tf.nn.conv2d_transpose(x, W, output_shape=output_shape,\n",
    "                                        strides=[1, sh, sw, 1])\n",
    "\n",
    "        biases = tf.get_variable('b', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        deconv_plus_b = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
    "\n",
    "        return deconv_plus_b\n",
    "\n",
    "\n",
    "def lrelu(x, leak=0.2):\n",
    "    return tf.maximum(x, leak * x)\n",
    "\n",
    "\n",
    "def fc(x, output_size, stddev=0.02, scope=\"fc\"):\n",
    "    with tf.variable_scope(scope):\n",
    "        shape = x.get_shape().as_list()\n",
    "        W = tf.get_variable(\"W\", [shape[1], output_size], tf.float32,\n",
    "                            tf.random_normal_initializer(stddev=stddev))\n",
    "        b = tf.get_variable(\"b\", [output_size],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        return tf.matmul(x, W) + b\n",
    "    \n",
    "def tf_init_embedding(size, dimension, stddev=0.01, scope=\"embedding\"):\n",
    "    with tf.variable_scope(scope):\n",
    "        return tf.get_variable(\"E\", [size, 1, 1, dimension], tf.float32,\n",
    "                               tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "\n",
    "def encoder(images, is_training=True, reuse=False):\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        encode_layers = dict()\n",
    "\n",
    "        def encode_layer(x, output_filters, layer):\n",
    "            act = lrelu(x)\n",
    "            conv = conv2d(act, output_filters=output_filters, scope=\"g_e%d_conv\" % layer)\n",
    "            enc = batch_norm(conv, is_training, scope=\"g_e%d_bn\" % layer)\n",
    "            encode_layers[\"e%d\" % layer] = enc\n",
    "            return enc\n",
    "\n",
    "        e1 = conv2d(images, generator_dim, scope=\"g_e1_conv\")\n",
    "        encode_layers[\"e1\"] = e1\n",
    "        e2 = encode_layer(e1, generator_dim * 2, 2)\n",
    "        e3 = encode_layer(e2, generator_dim * 4, 3)\n",
    "        e4 = encode_layer(e3, generator_dim * 8, 4)\n",
    "        e5 = encode_layer(e4, generator_dim * 8, 5)\n",
    "        e6 = encode_layer(e5, generator_dim * 8, 6)\n",
    "        e7 = encode_layer(e6, generator_dim * 8, 7)\n",
    "        e8 = encode_layer(e7, generator_dim * 8, 8)\n",
    "\n",
    "        return e8, encode_layers\n",
    "\n",
    "def decoder(encoded, encoding_layers, ids, inst_norm=False, is_training=True, reuse=False):\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        \n",
    "        decode_layers = dict()\n",
    "        output_width = 64\n",
    "        output_filters = 2\n",
    "        s = output_width\n",
    "        s2, s4, s8, s16, s32, s64 = int(s / 2), int(s / 4), int(s / 8), int(s / 16), int(s / 32), int(\n",
    "            s / 64)\n",
    "\n",
    "        def decode_layer(x, output_width, output_filters, layer, enc_layer, dropout=False, do_concat=True):\n",
    "            \n",
    "            dec = deconv2d(tf.nn.relu(x), [batch_size, output_width,\n",
    "                                           output_width, output_filters], scope=\"g_d%d_deconv\" % layer)\n",
    "            if layer != 8:\n",
    "                # IMPORTANT: normalization for last layer\n",
    "                # Very important, otherwise GAN is unstable\n",
    "                # Trying conditional instance normalization to\n",
    "                # overcome the fact that batch normalization offers\n",
    "                # different train/test statistics\n",
    "                if inst_norm:\n",
    "                    dec = conditional_instance_norm(dec, ids, embedding_num, scope=\"g_d%d_inst_norm\" % layer)\n",
    "                else:\n",
    "                    dec = batch_norm(dec, is_training, scope=\"g_d%d_bn\" % layer)\n",
    "            if dropout:\n",
    "                dec = tf.nn.dropout(dec, 0.5)\n",
    "            if do_concat:\n",
    "                dec = tf.concat([dec, enc_layer], 3)\n",
    "            decode_layers[\"d%d\" % layer] = dec\n",
    "            return dec\n",
    "\n",
    "        d1 = decode_layer(encoded, s64, generator_dim * 8, layer=1, enc_layer=encoding_layers[\"e7\"], dropout=True)\n",
    "        print(d1.shape)\n",
    "        d2 = decode_layer(d1, s64, generator_dim * 8, layer=2, enc_layer=encoding_layers[\"e6\"], dropout=True)\n",
    "        print(d2.shape)\n",
    "        d3 = decode_layer(d2, s32, generator_dim * 8, layer=3, enc_layer=encoding_layers[\"e5\"], dropout=True)\n",
    "        print(d3.shape)\n",
    "        d4 = decode_layer(d3, s16, generator_dim * 8, layer=4, enc_layer=encoding_layers[\"e4\"])\n",
    "        print(d4.shape)\n",
    "        d5 = decode_layer(d4, s8, generator_dim * 4, layer=5, enc_layer=encoding_layers[\"e3\"])\n",
    "        print(d5.shape)\n",
    "        d6 = decode_layer(d5, s4, generator_dim * 2, layer=6, enc_layer=encoding_layers[\"e2\"])\n",
    "        print(d6.shape)\n",
    "        d7 = decode_layer(d6, s2, generator_dim, layer=7, enc_layer=encoding_layers[\"e1\"])\n",
    "        print(d7.shape)\n",
    "        d8 = decode_layer(d7, s, output_filters, layer=8, enc_layer=None, do_concat=False)\n",
    "        print(d8.shape)\n",
    "\n",
    "        output = tf.nn.tanh(d8)  # scale to (-1, 1)\n",
    "        return output, decode_layers\n",
    "\n",
    "def generator(images, embeddings, embedding_ids, inst_norm, is_training, reuse=False):\n",
    "    e8, enc_layers = encoder(images, is_training=is_training, reuse=reuse)\n",
    "    local_embeddings = tf.nn.embedding_lookup(embeddings, ids=embedding_ids)\n",
    "    local_embeddings = tf.reshape(local_embeddings, [batch_size, 1, 1, embedding_dim])\n",
    "    embedded = tf.concat([e8, local_embeddings], 3)\n",
    "    output, _ = decoder(embedded, enc_layers, embedding_ids, inst_norm, is_training=is_training, reuse=reuse)\n",
    "    return output, e8\n",
    "\n",
    "def discriminator(image):\n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "        h0 = lrelu(conv2d(image, discriminator_dim, scope=\"d_h0_conv\"))\n",
    "        print(h0.shape)\n",
    "        h1 = lrelu(batch_norm(conv2d(h0, discriminator_dim * 2, scope=\"d_h1_conv\"),\n",
    "                              is_training=True, scope=\"d_bn_1\"))\n",
    "        print(h1.shape)\n",
    "        h2 = lrelu(batch_norm(conv2d(h1, discriminator_dim * 4, scope=\"d_h2_conv\"),\n",
    "                              is_training=True, scope=\"d_bn_2\"))\n",
    "        print(h2.shape)\n",
    "        h3 = lrelu(batch_norm(conv2d(h2, discriminator_dim * 8, sh=1, sw=1, scope=\"d_h3_conv\"),\n",
    "                              is_training=True, scope=\"d_bn_3\"))\n",
    "        print(h3.shape)\n",
    "        # real or fake binary loss\n",
    "        fc1 = fc(tf.reshape(h3, [batch_size, -1]), 1, scope=\"d_fc1\")\n",
    "        print(fc1.shape)\n",
    "        # category loss\n",
    "        fc2 = fc(tf.reshape(h3, [batch_size, -1]), embedding_num, scope=\"d_fc2\")\n",
    "        print(fc2.shape)\n",
    "\n",
    "        return tf.nn.sigmoid(fc1), fc1, fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/minn/.pyenv/versions/3.7.0/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generator_dim = 64\n",
    "tf_images = batch_images.reshape((32, 64, 64, 2))\n",
    "e8, tf_encode_layers = encoder(tf_images)\n",
    "# batch_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'generator/g_e8_bn/Identity:0' shape=(32, 1, 1, 512) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e1': <tf.Tensor 'generator/g_e1_conv/Reshape:0' shape=(32, 32, 32, 64) dtype=float32>,\n",
       " 'e2': <tf.Tensor 'generator/g_e2_bn/Identity:0' shape=(32, 16, 16, 128) dtype=float32>,\n",
       " 'e3': <tf.Tensor 'generator/g_e3_bn/Identity:0' shape=(32, 8, 8, 256) dtype=float32>,\n",
       " 'e4': <tf.Tensor 'generator/g_e4_bn/Identity:0' shape=(32, 4, 4, 512) dtype=float32>,\n",
       " 'e5': <tf.Tensor 'generator/g_e5_bn/Identity:0' shape=(32, 2, 2, 512) dtype=float32>,\n",
       " 'e6': <tf.Tensor 'generator/g_e6_bn/Identity:0' shape=(32, 1, 1, 512) dtype=float32>,\n",
       " 'e7': <tf.Tensor 'generator/g_e7_bn/Identity:0' shape=(32, 1, 1, 512) dtype=float32>,\n",
       " 'e8': <tf.Tensor 'generator/g_e8_bn/Identity:0' shape=(32, 1, 1, 512) dtype=float32>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_encode_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 64, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(23), Dimension(1), Dimension(1), Dimension(128)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_num = 23\n",
    "embedding_dim = 128\n",
    "embedding_ids = labels\n",
    "embeddings = tf_init_embedding(embedding_num, embedding_dim)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.ops.variables.RefVariable, list)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings), type(embedding_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 1, 128)\n"
     ]
    }
   ],
   "source": [
    "local_embeddings = tf.nn.embedding_lookup(embeddings, ids=embedding_ids)\n",
    "print(local_embeddings.shape)\n",
    "local_embeddings = tf.reshape(local_embeddings, [batch_size, 1, 1, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(32, 1, 1, 128) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(32, 1, 1, 640) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tf.concat([e8, local_embeddings], 3)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-71024b3c4e22>:107: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "(32, 1, 1, 1024)\n",
      "(32, 1, 1, 1024)\n",
      "(32, 2, 2, 1024)\n",
      "(32, 4, 4, 1024)\n",
      "(32, 8, 8, 512)\n",
      "(32, 16, 16, 256)\n",
      "(32, 32, 32, 128)\n",
      "(32, 64, 64, 2)\n"
     ]
    }
   ],
   "source": [
    "output, decode_layer = decoder(encoded, tf_encode_layers, ids=0, inst_norm=False, is_training=True, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32, 64)\n",
      "(32, 16, 16, 128)\n",
      "(32, 8, 8, 256)\n",
      "(32, 8, 8, 512)\n",
      "(32, 1)\n",
      "(32, 23)\n"
     ]
    }
   ],
   "source": [
    "discriminator_dim = 64\n",
    "embedding_num\n",
    "tf_loss, tf_loss_logit, category_loss = discriminator(tf_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(c_out, momentum=0.1):\n",
    "    return nn.BatchNorm2d(c_out, momentum=momentum)\n",
    "\n",
    "\n",
    "def conv2d(c_in, c_out, k_size=5, stride=2, pad=2, dilation=2, bn=True, lrelu=True, leak=0.2):\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    if lrelu:\n",
    "        layers.append(nn.LeakyReLU(leak))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def deconv2d(c_in, c_out, k_size=3, stride=1, pad=1, dilation=1, bn=True, dropout=True, p=0.5):\n",
    "    layers = []\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    if dropout:\n",
    "        layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.LeakyReLU(0.1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def lrelu(leak=0.2):\n",
    "    return nn.LeakyReLU(leak)\n",
    "\n",
    "\n",
    "def dropout(p=0.2):\n",
    "    return nn.Dropout(p)\n",
    "\n",
    "\n",
    "def fc(input_size, output_size):\n",
    "    return nn.Linear(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_dim=2, conv_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = conv2d(img_dim, conv_dim)\n",
    "        self.conv2 = conv2d(conv_dim, conv_dim*2)\n",
    "        self.conv3 = conv2d(conv_dim*2, conv_dim*4)\n",
    "        self.conv4 = conv2d(conv_dim*4, conv_dim*8)\n",
    "        self.conv5 = conv2d(conv_dim*8, conv_dim*8)\n",
    "        self.conv6 = conv2d(conv_dim*8, conv_dim*8)\n",
    "        self.conv7 = conv2d(conv_dim*8, conv_dim*8)\n",
    "        self.conv8 = conv2d(conv_dim*8, conv_dim*8)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        encode_layers = dict()\n",
    "        \n",
    "        e1 = self.conv1(images)\n",
    "        encode_layers['e1'] = e1\n",
    "        e2 = self.conv2(e1)\n",
    "        encode_layers['e2'] = e2\n",
    "        e3 = self.conv3(e2)\n",
    "        encode_layers['e3'] = e3\n",
    "        e4 = self.conv4(e3)\n",
    "        encode_layers['e4'] = e4\n",
    "        e5 = self.conv5(e4)\n",
    "        encode_layers['e5'] = e5\n",
    "        e6 = self.conv6(e5)\n",
    "        encode_layers['e6'] = e6\n",
    "        e7 = self.conv7(e6)\n",
    "        encode_layers['e7'] = e7\n",
    "        encoded_images = self.conv8(e7)\n",
    "        encode_layers['e8'] = encoded_images\n",
    "        \n",
    "        return encoded_images, encode_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTS_NUM = 30\n",
    "EMBEDDING_DIM = 128\n",
    "BATCH_SIZE = 32\n",
    "embedding_num = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 1, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDINGS = init_embedding(FONTS_NUM, EMBEDDING_DIM)\n",
    "embedding_ids = labels\n",
    "local_embeddings = embedding_lookup(EMBEDDINGS, embedding_ids)\n",
    "local_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(2, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv6): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv7): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv8): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dim = 2\n",
    "En = Encoder()\n",
    "En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images, fake_images = En(batch_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 512, 1, 1]), torch.Size([32, 2, 64, 64]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_images.shape, fake_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding(embedding_num, embedding_dim, stddev=0.01):\n",
    "    embedding = torch.randn(embedding_num, embedding_dim) * stddev\n",
    "    embedding = embedding.reshape((embedding_num, 1, 1, embedding_dim))\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 1, 1, 128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_num = 23\n",
    "embedding_dim = 128\n",
    "embeddings = init_embedding(embedding_num, embedding_dim)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_ids = labels\n",
    "len(embedding_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(embeddings, embedding_ids):\n",
    "    local_embeddings = []\n",
    "    for id_ in embedding_ids:\n",
    "        local_embeddings.append(embeddings[id_].data.numpy())\n",
    "    local_embeddings = torch.from_numpy(np.array(local_embeddings))\n",
    "    local_embeddings = local_embeddings.reshape(batch_size, embedding_dim, 1, 1)\n",
    "    return local_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 1, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_embeddings = embedding_lookup(embeddings, embedding_ids)\n",
    "local_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e8.shape, local_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = torch.cat((e8, local_embeddings), 1)\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedded_dim=640, conv_dim=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.deconv1 = deconv2d(embedded_dim, conv_dim*8)\n",
    "        self.deconv2 = deconv2d(conv_dim*16, conv_dim*8)\n",
    "        self.deconv3 = deconv2d(conv_dim*16, conv_dim*8, k_size=4, dilation=2)\n",
    "        self.deconv4 = deconv2d(conv_dim*16, conv_dim*8, k_size=5, dilation=2)\n",
    "        self.deconv5 = deconv2d(conv_dim*16, conv_dim*4, k_size=4, dilation=2, stride=2)\n",
    "        self.deconv6 = deconv2d(conv_dim*8, conv_dim*2, k_size=4, dilation=2, stride=2)\n",
    "        self.deconv7 = deconv2d(conv_dim*4, conv_dim*1, k_size=4, dilation=2, stride=2)\n",
    "        self.deconv8 = deconv2d(conv_dim*2, image_dim, k_size=4, dilation=2, stride=2, bn=False)\n",
    "    \n",
    "    \n",
    "    def forward(self, embedded, encode_layers):\n",
    "        decode_layers = dict()\n",
    "        \n",
    "        d1 = self.deconv1(embedded)\n",
    "        d1 = torch.cat((d1, encode_layers['e7']), dim=1)\n",
    "        d2 = self.deconv2(d1)\n",
    "        d2 = torch.cat((d2, encode_layers['e6']), dim=1)\n",
    "        d3 = self.deconv3(d2)\n",
    "        d3 = torch.cat((d3, encode_layers['e5']), dim=1)\n",
    "        d4 = self.deconv4(d3)\n",
    "        d4 = torch.cat((d4, encode_layers['e4']), dim=1)\n",
    "        d5 = self.deconv5(d4)\n",
    "        d5 = torch.cat((d5, encode_layers['e3']), dim=1)\n",
    "        d6 = self.deconv6(d5)\n",
    "        d6 = torch.cat((d6, encode_layers['e2']), dim=1)\n",
    "        d7 = self.deconv7(d6)\n",
    "        d7 = torch.cat((d7, encode_layers['e1']), dim=1)\n",
    "        d8 = self.deconv8(d7)        \n",
    "        fake_images = torch.tanh(d8)\n",
    "        \n",
    "        decode_layers['d1'] = d1\n",
    "        decode_layers['d2'] = d2\n",
    "        decode_layers['d3'] = d3\n",
    "        decode_layers['d4'] = d4\n",
    "        decode_layers['d5'] = d5\n",
    "        decode_layers['d6'] = d6\n",
    "        decode_layers['d7'] = d7\n",
    "        decode_layers['d8'] = d8\n",
    "        \n",
    "        return fake_images, decode_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "De = Decoder()\n",
    "De"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d8, decode_layers = De(embedded, encode_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedded.shape)\n",
    "for key, value in decode_layers.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(images, embeddings, embedding_ids):\n",
    "    encoded_images, encode_layers = En(images)\n",
    "    local_embeddings = embedding_lookup(embeddings, embedding_ids)\n",
    "    embedded = torch.cat((e8, local_embeddings), 1)\n",
    "    fake_images, decode_layers = De(embedded, encode_layers)\n",
    "    return fake_images, encoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_image, encoded_image = generator(images, embeddings, embedding_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1, 1))\n",
    "plt.imshow(fake_image[0][0].data, cmap='gray')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_image.data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_image[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e8.data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embedding_num, img_dim=2, disc_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = conv2d(img_dim, disc_dim, bn=False)\n",
    "        self.conv2 = conv2d(disc_dim, disc_dim*2)\n",
    "        self.conv3 = conv2d(disc_dim*2, disc_dim*4)\n",
    "        self.conv4 = conv2d(disc_dim*4, disc_dim*8, stride=1)\n",
    "        self.fc1 = fc(disc_dim*8*8*8, 1)\n",
    "        self.fc2 = fc(disc_dim*8*8*8, embedding_num)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        h1 = self.conv1(images)\n",
    "        h2 = self.conv2(h1)\n",
    "        h3 = self.conv3(h2)\n",
    "        h4 = self.conv4(h3)\n",
    "        \n",
    "        tf_loss_logit = self.fc1(h4.reshape(batch_size, -1))\n",
    "        tf_loss = torch.sigmoid(tf_loss_logit)\n",
    "        cat_loss = self.fc2(h4.reshape(batch_size, -1))\n",
    "        \n",
    "        return tf_loss, tf_loss_logit, cat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator(embedding_num=23)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_loss, tf_loss_logit, cat_loss = D(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_loss.shape, tf_loss_logit.shape, cat_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
