{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, time, datetime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from common.dataset import TrainDataProvider\n",
    "from common.function import init_embedding\n",
    "from common.models import Encoder, Decoder, Discriminator, Generator\n",
    "from common.utils import denorm_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Mindset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제상황\n",
    "1. 이전의 실험에서 learning rate 때문인지, 무슨이유인지 몰라도 mode collapsing이 발생함\n",
    "\n",
    "\n",
    "2. 각 폰트가 각자의 label에 맞지 않게 학습이 되는 것으로 보임\n",
    "\n",
    "\n",
    "### 수정사항\n",
    "1. 일단은 learning rate를 다시 잡아가면서 학습시켜보기로 함\n",
    "\n",
    "\n",
    "2. 그리고 데이터를 모든 폰트별로 11712자를 전부 학습시킬 필요는 없는 것 같고, 그 중 `2000자만 랜덤`으로 뽑아서 사용하도록 하겠음\n",
    "\n",
    "\n",
    "3. 또, `batch_size=16`으로 줄여서 학습하기\n",
    "\n",
    "### 다시 시작하는 지점\n",
    "1. 이전에 lr=0.001로 학습시켰던 모델 중 epoch 11까지 학습된 모델에서 다시 시작\n",
    "\n",
    "\n",
    "2. 그 이후, `20epoch`까지 lr=0.0005로, 30epoch까지 lr=0.00025로 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 다시 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import pickle as pickle\n",
    "import random\n",
    "\n",
    "\n",
    "def pickle_examples(from_dir, train_path, val_path, train_val_split=0.2):\n",
    "    \"\"\"\n",
    "    Compile a list of examples into pickled format, so during\n",
    "    the training, all io will happen in memory\n",
    "    \"\"\"\n",
    "    paths = glob.glob(os.path.join(from_dir, \"*.png\"))\n",
    "    with open(train_path, 'wb') as ft:\n",
    "        with open(val_path, 'wb') as fv:\n",
    "            print('all data num:', len(paths))\n",
    "            c = 1\n",
    "            val_count = 0\n",
    "            train_count = 0\n",
    "            for p in paths:\n",
    "                c += 1\n",
    "                label = int(os.path.basename(p).split(\"_\")[0])\n",
    "                with open(p, 'rb') as f:\n",
    "                    img_bytes = f.read()\n",
    "                    example = (label, img_bytes)\n",
    "                    r = random.random()\n",
    "                    if r < train_val_split:\n",
    "                        pickle.dump(example, fv)\n",
    "                        val_count += 1\n",
    "                        if val_count % 10000 == 0:\n",
    "                            print(\"%d imgs saved in val.obj\" % val_count)\n",
    "                    else:\n",
    "                        pickle.dump(example, ft)\n",
    "                        train_count += 1\n",
    "                        if train_count % 10000 == 0:\n",
    "                            print(\"%d imgs saved in train.obj\" % train_count)\n",
    "            print(\"%d imgs saved in val.obj, end\" % val_count)\n",
    "            print(\"%d imgs saved in train.obj, end\" % train_count)\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 75000개 수준의 dataset으로 작게 만들어서 다시 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all data num: 226659\n",
      "10000 imgs saved in val.obj\n",
      "20000 imgs saved in val.obj\n",
      "10000 imgs saved in train.obj\n",
      "30000 imgs saved in val.obj\n",
      "40000 imgs saved in val.obj\n",
      "20000 imgs saved in train.obj\n",
      "50000 imgs saved in val.obj\n",
      "60000 imgs saved in val.obj\n",
      "30000 imgs saved in train.obj\n",
      "70000 imgs saved in val.obj\n",
      "80000 imgs saved in val.obj\n",
      "40000 imgs saved in train.obj\n",
      "90000 imgs saved in val.obj\n",
      "100000 imgs saved in val.obj\n",
      "50000 imgs saved in train.obj\n",
      "110000 imgs saved in val.obj\n",
      "120000 imgs saved in val.obj\n",
      "60000 imgs saved in train.obj\n",
      "130000 imgs saved in val.obj\n",
      "140000 imgs saved in val.obj\n",
      "70000 imgs saved in train.obj\n",
      "150000 imgs saved in val.obj\n",
      "151911 imgs saved in val.obj, end\n",
      "74748 imgs saved in train.obj, end\n"
     ]
    }
   ],
   "source": [
    "from_dir = './get_data/hangul-dataset-11172/'\n",
    "save_dir = './dataset/'\n",
    "train_path = os.path.join(save_dir, \"train.obj\")\n",
    "val_path = os.path.join(save_dir, \"val.obj\")\n",
    "\n",
    "pickle_examples(from_dir, train_path=train_path, val_path=val_path, train_val_split=1-0.33) # 75000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> training dataset : `74748`\n",
    "\n",
    "- dataset spec: `25fonts`, `3000chars /fonts`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU = torch.cuda.is_available()\n",
    "GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dataset/'\n",
    "model_dir = './model_save/'\n",
    "fixed_dir = './fixed_sample'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Fixed Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 1, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.load(os.path.join(fixed_dir, 'EMBEDDINGS.pkl'))\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Fixed sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed sources: 26\n",
      "fixed targets: 26\n",
      "fixed labels: 26\n"
     ]
    }
   ],
   "source": [
    "fixed_sources, fixed_targets, fixed_labels = [], [], []\n",
    "\n",
    "# font별 fixed target\n",
    "for i in range(25):\n",
    "    source = torch.load(os.path.join(fixed_dir, 'fixed_source_%d.pkl' % i))\n",
    "    target = torch.load(os.path.join(fixed_dir, 'fixed_target_%d.pkl' % i))\n",
    "    label = torch.load(os.path.join(fixed_dir, 'fixed_label_%d.pkl' % i))\n",
    "    fixed_sources.append(source)\n",
    "    fixed_targets.append(target)\n",
    "    fixed_labels.append(label)\n",
    "    \n",
    "# 모든 폰트가 섞여있는 target\n",
    "source = torch.load(os.path.join(fixed_dir, 'fixed_source_all.pkl'))\n",
    "target = torch.load(os.path.join(fixed_dir, 'fixed_target_all.pkl'))\n",
    "label = torch.load(os.path.join(fixed_dir, 'fixed_label_all.pkl'))\n",
    "fixed_sources.append(source)\n",
    "fixed_targets.append(target)\n",
    "fixed_labels.append(label)\n",
    "\n",
    "print(\"fixed sources:\", len(fixed_sources))\n",
    "print(\"fixed targets:\", len(fixed_targets))\n",
    "print(\"fixed labels:\", len(fixed_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixed_source는 일단 폰트 다 섞여있는 걸로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_source = fixed_sources[-1]\n",
    "fixed_target = fixed_targets[-1]\n",
    "fixed_label = fixed_labels[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickled total 74748 examples\n",
      "unpickled total 151911 examples\n",
      "train examples -> 74748, val examples -> 151911\n",
      "total batches: 4672\n"
     ]
    }
   ],
   "source": [
    "data_provider = TrainDataProvider(data_dir)\n",
    "total_batches = data_provider.compute_total_batch_num(BATCH_SIZE)\n",
    "print(\"total batches:\", total_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch_size 16으로 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTS_NUM = 25\n",
    "EMBEDDING_NUM = 100\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 128\n",
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_epoch, schedule, data_dir, save_path, to_model_path, lr=0.001, \\\n",
    "          log_step=100, sample_step=350, fine_tune=False, flip_labels=False, \\\n",
    "          restore=None, from_model_path=False, GPU=True):\n",
    "    \n",
    "    # Fine Tuning coefficient\n",
    "    if not fine_tune:\n",
    "        L1_penalty, Lconst_penalty = 100, 15\n",
    "    else:\n",
    "        L1_penalty, Lconst_penalty = 500, 1000\n",
    "\n",
    "    # Get Models\n",
    "    En = Encoder()\n",
    "    De = Decoder()\n",
    "    D = Discriminator(category_num=FONTS_NUM)\n",
    "    if GPU:\n",
    "        En.cuda()\n",
    "        De.cuda()\n",
    "        D.cuda()\n",
    "    \n",
    "    # Use pre-trained Model\n",
    "    # restore에 [encoder_path, decoder_path, discriminator_path] 형태로 인자 넣기\n",
    "    if restore:\n",
    "        encoder_path, decoder_path, discriminator_path = restore\n",
    "        prev_epoch = int(encoder_path.split('-')[0])\n",
    "        En.load_state_dict(torch.load(os.path.join(from_model_path, encoder_path)))\n",
    "        De.load_state_dict(torch.load(os.path.join(from_model_path, decoder_path)))\n",
    "        D.load_state_dict(torch.load(os.path.join(from_model_path, discriminator_path)))\n",
    "        print(\"%d epoch trained model has restored\" % prev_epoch)\n",
    "    else:\n",
    "        prev_epoch = 0\n",
    "        print(\"New model training start\")\n",
    "\n",
    "        \n",
    "    # L1 loss, binary real/fake loss, category loss, constant loss\n",
    "    if GPU:\n",
    "        l1_criterion = nn.L1Loss(size_average=True).cuda()\n",
    "        bce_criterion = nn.BCEWithLogitsLoss(size_average=True).cuda()\n",
    "        mse_criterion = nn.MSELoss(size_average=True).cuda()\n",
    "    else:\n",
    "        l1_criterion = nn.L1Loss(size_average=True)\n",
    "        bce_criterion = nn.BCEWithLogitsLoss(size_average=True)\n",
    "        mse_criterion = nn.MSELoss(size_average=True)\n",
    "\n",
    "\n",
    "    # optimizer\n",
    "    G_parameters = list(En.parameters()) + list(De.parameters())\n",
    "    g_optimizer = torch.optim.Adam(G_parameters, betas=(0.5, 0.999))\n",
    "    d_optimizer = torch.optim.Adam(D.parameters(), betas=(0.5, 0.999))\n",
    "    \n",
    "    # losses lists\n",
    "    l1_losses, const_losses, category_losses, d_losses, g_losses = list(), list(), list(), list(), list()\n",
    "    \n",
    "    # training\n",
    "    count = 0\n",
    "    for epoch in range(max_epoch):\n",
    "        if (epoch + 1) % schedule == 0:\n",
    "            updated_lr = max(lr/2, 0.0002)\n",
    "            for param_group in d_optimizer.param_groups:\n",
    "                param_group['lr'] = updated_lr\n",
    "            for param_group in g_optimizer.param_groups:\n",
    "                param_group['lr'] = updated_lr\n",
    "            if lr !=  updated_lr:\n",
    "                print(\"decay learning rate from %.5f to %.5f\" % (lr, updated_lr))\n",
    "            lr = updated_lr\n",
    "            \n",
    "        train_batch_iter = data_provider.get_train_iter(BATCH_SIZE)   \n",
    "        for i, batch in enumerate(train_batch_iter):\n",
    "            labels, batch_images = batch\n",
    "            embedding_ids = labels\n",
    "            if GPU:\n",
    "                batch_images = batch_images.cuda()\n",
    "            if flip_labels:\n",
    "                np.random.shuffle(embedding_ids)\n",
    "                \n",
    "            # target / source images\n",
    "            real_target = batch_images[:, 0, :, :].view([BATCH_SIZE, 1, IMG_SIZE, IMG_SIZE])\n",
    "            real_source = batch_images[:, 1, :, :].view([BATCH_SIZE, 1, IMG_SIZE, IMG_SIZE])\n",
    "            \n",
    "            # generate fake image form source image\n",
    "            fake_target, encoded_source = Generator(real_source, En, De, embeddings, embedding_ids, GPU=GPU)\n",
    "            \n",
    "            real_TS = torch.cat([real_source, real_target], dim=1)\n",
    "            fake_TS = torch.cat([real_source, fake_target], dim=1)\n",
    "            \n",
    "            # Scoring with Discriminator\n",
    "            real_score, real_score_logit, real_cat_logit = D(real_TS)\n",
    "            fake_score, fake_score_logit, fake_cat_logit = D(fake_TS)\n",
    "            \n",
    "            # Get encoded fake image to calculate constant loss\n",
    "            encoded_fake = En(fake_target)[0]\n",
    "            const_loss = Lconst_penalty * mse_criterion(encoded_source, encoded_fake)\n",
    "            \n",
    "            # category loss\n",
    "            real_category = torch.from_numpy(np.eye(FONTS_NUM)[embedding_ids]).float()\n",
    "            if GPU:\n",
    "                real_category = real_category.cuda()\n",
    "            real_category_loss = bce_criterion(real_cat_logit, real_category)\n",
    "            fake_category_loss = bce_criterion(fake_cat_logit, real_category)\n",
    "            category_loss = 0.5 * (real_category_loss + fake_category_loss)\n",
    "            \n",
    "            # labels\n",
    "            if GPU:\n",
    "                one_labels = torch.ones([BATCH_SIZE, 1]).cuda()\n",
    "                zero_labels = torch.zeros([BATCH_SIZE, 1]).cuda()\n",
    "            else:\n",
    "                one_labels = torch.ones([BATCH_SIZE, 1])\n",
    "                zero_labels = torch.zeros([BATCH_SIZE, 1])\n",
    "            \n",
    "            # binary loss - T/F\n",
    "            real_binary_loss = bce_criterion(real_score_logit, one_labels)\n",
    "            fake_binary_loss = bce_criterion(fake_score_logit, zero_labels)\n",
    "            binary_loss = real_binary_loss + fake_binary_loss\n",
    "            \n",
    "            # L1 loss between real and fake images\n",
    "            l1_loss = L1_penalty * l1_criterion(real_target, fake_target)\n",
    "            \n",
    "            # cheat loss for generator to fool discriminator\n",
    "            cheat_loss = bce_criterion(fake_score_logit, one_labels)\n",
    "            \n",
    "            # g_loss, d_loss\n",
    "            g_loss = cheat_loss + l1_loss + fake_category_loss + const_loss\n",
    "            d_loss = binary_loss + category_loss\n",
    "            \n",
    "            # train Discriminator\n",
    "            D.zero_grad()\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # train Generator\n",
    "            En.zero_grad()\n",
    "            De.zero_grad()\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            g_optimizer.step()            \n",
    "            \n",
    "            # loss data\n",
    "            l1_losses.append(l1_loss.data)\n",
    "            const_losses.append(const_loss.data)\n",
    "            category_losses.append(category_loss.data)\n",
    "            d_losses.append(d_loss.data)\n",
    "            g_losses.append(g_loss.data)\n",
    "            \n",
    "            # logging\n",
    "            if (i+1) % log_step == 0:\n",
    "                time_ = time.time()\n",
    "                time_stamp = datetime.datetime.fromtimestamp(time_).strftime('%H:%M:%S')\n",
    "                log_format = 'Epoch [%d/%d], step [%d/%d], l1_loss: %.4f, d_loss: %.4f, g_loss: %.4f' % \\\n",
    "                             (int(prev_epoch)+epoch+1, int(prev_epoch)+max_epoch, i+1, total_batches, \\\n",
    "                              l1_loss.item(), d_loss.item(), g_loss.item())\n",
    "                print(time_stamp, log_format)\n",
    "                \n",
    "            # save image\n",
    "            if (i+1) % sample_step == 0:\n",
    "                fixed_fake_images = Generator(fixed_source, En, De, embeddings, fixed_label, GPU=GPU)[0]\n",
    "                save_image(denorm_image(fixed_fake_images.data), \\\n",
    "                           os.path.join(save_path, 'fake_samples-%d-%d.png' % (int(prev_epoch)+epoch+1, i+1)), \\\n",
    "                           nrow=8)\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            now = datetime.datetime.now()\n",
    "            now_date = now.strftime(\"%m%d\")\n",
    "            now_time = now.strftime('%H:%M')\n",
    "            torch.save(En.state_dict(), os.path.join(to_model_path, '%d-%s-%s-Encoder.pkl' \\\n",
    "                                                     % (int(prev_epoch)+epoch+1, now_date, now_time)))\n",
    "            torch.save(De.state_dict(), os.path.join(to_model_path, '%d-%s-%s-Decoder.pkl' % \\\n",
    "                                                     (int(prev_epoch)+epoch+1, now_date, now_time)))\n",
    "            torch.save(D.state_dict(), os.path.join(to_model_path, '%d-%s-%s-Discriminator.pkl' % \\\n",
    "                                                    (int(prev_epoch)+epoch+1, now_date, now_time)))\n",
    "\n",
    "    # save model\n",
    "    total_epoch = int(prev_epoch) + int(max_epoch)\n",
    "    end = datetime.datetime.now()\n",
    "    end_date = end.strftime(\"%m%d\")\n",
    "    end_time = end.strftime('%H:%M')\n",
    "    torch.save(En.state_dict(), os.path.join(to_model_path, \\\n",
    "                                             '%d-%s-%s-Encoder.pkl' % (total_epoch, end_date, end_time)))\n",
    "    torch.save(De.state_dict(), os.path.join(to_model_path, \\\n",
    "                                             '%d-%s-%s-Decoder.pkl' % (total_epoch, end_date, end_time)))\n",
    "    torch.save(D.state_dict(), os.path.join(to_model_path, \\\n",
    "                                            '%d-%s-%s-Discriminator.pkl' % (total_epoch, end_date, end_time)))\n",
    "    losses = [l1_losses, const_losses, category_losses, d_losses, g_losses]\n",
    "    torch.save(losses, os.path.join(to_model_path, '%d-losses.pkl' % max_epoch))\n",
    "\n",
    "    return l1_losses, const_losses, category_losses, d_losses, g_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lr=0.001` 10epoch / `lr=0.0005` 11~20epoch / `lr=0.00025` 21~30epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model training start\n",
      "13:42:34 Epoch [1/30], step [500/4672], l1_loss: 29.0623, d_loss: 0.9056, g_loss: 33.6557\n",
      "13:44:05 Epoch [1/30], step [1000/4672], l1_loss: 31.8920, d_loss: 0.7461, g_loss: 34.6132\n",
      "13:45:36 Epoch [1/30], step [1500/4672], l1_loss: 30.8913, d_loss: 0.8861, g_loss: 35.2432\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-39a623606466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mto_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./model_checkpoint/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m losses = train(max_epoch=30, schedule=10, data_dir=data_dir, save_path=save_path, \\\n\u001b[0;32m----> 4\u001b[0;31m                to_model_path=to_model_path, log_step=500, sample_step=500)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-1b26533eb75b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(max_epoch, schedule, data_dir, save_path, to_model_path, lr, log_step, sample_step, fine_tune, flip_labels, restore, from_model_path, GPU)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0membedding_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mGPU\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mbatch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflip_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_path = './fixed_fake/'\n",
    "to_model_path = './model_checkpoint/'\n",
    "losses = train(max_epoch=30, schedule=10, data_dir=data_dir, save_path=save_path, \\\n",
    "               to_model_path=to_model_path, log_step=500, sample_step=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 약 14분/1epoch : 2시간20분/10epoch\n",
    "\n",
    "\n",
    "- 7시간/30epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
